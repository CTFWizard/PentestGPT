import textwrap
PARSING_CHAR_WINDOW = 16000

class ParsingModule:
    """ Parsing module responsible for parsing user and tool input for pentestgpt
    """
    postfix_options = {
        "tool": "The input content is from a security testing tool. You need to list down all the points that are interesting to you; you should summarize it as if you are reporting to a senior penetration tester for further guidance.\n",
        "user-comments": "The input content is from user comments.\n",
        "web": "The input content is from web pages. You need to summarize the readable-contents, and list down all the points that can be interesting for penetration testing.\n",
        "default": "The user did not specify the input source. You need to summarize based on the contents.\n",
    }

    options_desc = {
        "tool": " Paste the output of the security test tool used",
        "user-comments": "",
        "web": " Paste the relevant content of a web page",
        "deafult": " Write whatever you want, the tool will handle it",
    }
    def __init__(self, parsing_model, input_parsing_session_id=None):
        self.parsing_char_window = PARSING_CHAR_WINDOW        # LLM context window size
        self.parsing_model = parsing_model      # LLM model
        self.input_parsing_session_id = input_parsing_session_id    # Session Id

    # #TODO: discuss: point of this is to prevent race condition on LLM side. Unecessary atm.
    # def __new__(cls):
    #     """Singleton pattern: If there exists a parsing module object, return that object
    #     """
    #     if not hasattr(cls, 'instance'):
    #         cls.instance = super(ParsingModule, cls).__new__(cls)
    #     return cls.instance

    def input_parsing_handler(self, text, source=None) -> str:
        prefix = "Please summarize the following input. "
        # do some engineering trick here. Add postfix to the input to make it more understandable by LLMs.
        if source is not None and source in self.postfix_options.keys():
            prefix += self.postfix_options[source]
        # The default token-size limit is 4096 (web UI even shorter). 1 token ~= 4 chars in English
        # Use textwrap to split inputs. Limit to 2000 token (8000 chars) for each input
        # (1) replace all the newlines with spaces
        text = text.replace("\r", " ").replace("\n", " ")
        # (2) wrap the text
        wrapped_text = textwrap.fill(text, 8000)
        wrapped_inputs = wrapped_text.split("\n")
        # (3) send the inputs to chatGPT input_parsing_session and obtain the results
        summarized_content = ""
        for wrapped_input in wrapped_inputs:
            word_limit = f"Please ensure that the input is less than {8000 / len(wrapped_inputs)} words.\n"
            summarized_content += self.parsing_model.send_message(
                prefix + word_limit + wrapped_input, self.input_parsing_session_id
            )
        # log the conversation
        # self.log_conversation("input_parsing", summarized_content) #TODO: Fix logging
        return summarized_content